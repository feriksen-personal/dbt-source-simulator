# dbt Profiles for dbt-azure-demo-source-ops
#
# Copy this file to ~/.dbt/profiles.yml and customize for your environment.
#
# Usage:
#   dbt run-operation origin_load_baseline --profile ingestion_simulator               # Uses default 'dev' target
#   dbt run-operation origin_load_baseline --profile ingestion_simulator --target motherduck
#   dbt run-operation origin_load_baseline --profile ingestion_simulator --target azure
#   dbt run-operation origin_load_baseline --profile ingestion_simulator --target databricks
#
# Documentation: https://github.com/feriksen-personal/dbt-azure-demo-source-ops/wiki/Getting-Started

ingestion_simulator:
  target: dev  # Default target (change to motherduck, azure, or databricks if preferred)

  outputs:
    # ========================================================================
    # Local DuckDB - Recommended for getting started
    # ========================================================================
    # Use case: Local development, zero cloud costs, fastest iteration
    # Requirements: pip install dbt-duckdb
    # Database files: Created in the path specified below
    dev:
      type: duckdb
      path: 'data/ingestion_simulator.duckdb'  # Relative or absolute path
      threads: 4

    # ========================================================================
    # MotherDuck - Cloud collaboration
    # ========================================================================
    # Use case: Cloud collaboration, shared databases, Databricks Asset Bundles
    # Requirements:
    #   - pip install dbt-duckdb
    #   - MotherDuck account (free tier available): https://motherduck.com
    #   - Service token from https://motherduck.com (Settings â†’ Service Tokens)
    #
    # Setup:
    #   export MOTHERDUCK_TOKEN=your-motherduck-service-token-here
    #
    # Testing connection:
    #   dbt debug --profile ingestion_simulator --target motherduck
    motherduck:
      type: duckdb
      path: 'md:'  # Special path for MotherDuck - databases created automatically
      token: "{{ env_var('MOTHERDUCK_TOKEN') }}"
      threads: 4

    # ========================================================================
    # Azure SQL - Production-like cloud demos
    # ========================================================================
    # Use case: Cloud demos, CDC demonstrations, change tracking
    # Requirements:
    #   - pip install dbt-sqlserver
    #   - Azure SQL database provisioned
    #   - ODBC Driver 18 for SQL Server installed
    #
    # Setup environment variables:
    #   export SQL_SERVER=your-server-name  # Without .database.windows.net
    #   export SQL_USER=your-username
    #   export SQL_PASSWORD=your-password
    #
    # Testing connection:
    #   dbt debug --profile ingestion_simulator --target azure
    #
    # Note: Package creates jaffle_shop and jaffle_crm databases automatically
    azure:
      type: sqlserver
      server: "{{ env_var('SQL_SERVER') }}.database.windows.net"
      port: 1433
      database: master  # Package creates jaffle_shop/jaffle_crm databases
      schema: dbo
      authentication: sql
      user: "{{ env_var('SQL_USER') }}"
      password: "{{ env_var('SQL_PASSWORD') }}"
      driver: "ODBC Driver 18 for SQL Server"
      encrypt: true          # Required for Azure SQL
      trust_cert: false      # Required for Azure SQL
      threads: 4

    # ========================================================================
    # Databricks - Unity Catalog & Delta Lake
    # ========================================================================
    # Use case: Lakehouse workflows, Delta Lake demos, Unity Catalog testing
    # Requirements:
    #   - pip install dbt-databricks
    #   - Databricks workspace with SQL Warehouse
    #   - Personal access token
    #
    # Setup environment variables:
    #   export DATABRICKS_SERVER_HOSTNAME=your-workspace.cloud.databricks.com
    #   export DATABRICKS_HTTP_PATH=/sql/1.0/warehouses/your-warehouse-id
    #   export DATABRICKS_TOKEN=your-personal-access-token
    #   export DATABRICKS_CATALOG=your-catalog-name
    #
    # Testing connection:
    #   dbt debug --profile ingestion_simulator --target databricks
    #
    # Note: Creates schemas 'erp' (shop tables) and 'crm' (CRM tables) in your catalog
    databricks:
      type: databricks
      host: "{{ env_var('DATABRICKS_SERVER_HOSTNAME') }}"
      http_path: "{{ env_var('DATABRICKS_HTTP_PATH') }}"
      token: "{{ env_var('DATABRICKS_TOKEN') }}"
      catalog: "{{ env_var('DATABRICKS_CATALOG') }}"
      schema: default  # Package creates 'erp' and 'crm' schemas
      threads: 4

# ============================================================================
# Environment Variable Setup
# ============================================================================
#
# Create a .env file in your project root (add to .gitignore):
#
#   # MotherDuck
#   export MOTHERDUCK_TOKEN=your-token-here
#
#   # Azure SQL
#   export SQL_SERVER=your-server-name
#   export SQL_USER=your-username
#   export SQL_PASSWORD=your-password
#
#   # Databricks
#   export DATABRICKS_SERVER_HOSTNAME=your-workspace.cloud.databricks.com
#   export DATABRICKS_HTTP_PATH=/sql/1.0/warehouses/your-warehouse-id
#   export DATABRICKS_TOKEN=your-personal-access-token
#   export DATABRICKS_CATALOG=your-catalog-name
#
# Then source it before running dbt:
#   source .env
#   dbt run-operation origin_load_baseline --profile ingestion_simulator
#
# ============================================================================
# Testing Your Configuration
# ============================================================================
#
# Test each target:
#   dbt debug --profile ingestion_simulator --target dev
#   dbt debug --profile ingestion_simulator --target motherduck
#   dbt debug --profile ingestion_simulator --target azure
#   dbt debug --profile ingestion_simulator --target databricks
#
# Expected output:
#   Configuration:
#     profiles.yml file [OK found and valid]
#     dbt_project.yml file [OK found and valid]
#   Connection test: [OK connection ok]
#
# ============================================================================
# Troubleshooting
# ============================================================================
#
# Issue: "Profile ingestion_simulator does not exist"
# Solution: Ensure this file is at ~/.dbt/profiles.yml
#
# Issue: "Could not find profile named 'ingestion_simulator'"
# Solution: Check YAML indentation (use spaces, not tabs)
#
# Issue: MotherDuck "token is required"
# Solution: Ensure MOTHERDUCK_TOKEN environment variable is set
#
# Issue: Azure SQL connection timeout
# Solution: Check firewall rules allow your IP address
#
# For more help: https://github.com/feriksen-personal/dbt-azure-demo-source-ops/wiki/Troubleshooting
